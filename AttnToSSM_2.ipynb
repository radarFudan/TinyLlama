{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiops/wangsd/miniforge3/envs/TL/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/aiops/wangsd/miniforge3/envs/TL/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/aiops/wangsd/miniforge3/envs/TL/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiops/wangsd/miniforge3/envs/TL/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/aiops/wangsd/miniforge3/envs/TL did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/aiops/wangsd/miniforge3/envs/TL/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/aiops/wangsd/miniforge3/etc/xml/catalog file'), PosixPath('/etc/xml/catalog'), PosixPath('file')}\n",
      "  warn(msg)\n",
      "/home/aiops/wangsd/miniforge3/envs/TL/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/aiops/wangsd/miniforge3/envs/TL/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/aiops/wangsd/miniforge3/envs/TL/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu121 with CUDA None (you have 2.1.1+cu118)\n",
      "    Python  3.11.6 (you have 3.11.6)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Union\n",
    "import math\n",
    "import lightning as L\n",
    "import torch\n",
    "from lightning.fabric.strategies import FSDPStrategy, XLAStrategy\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "# support running without installing as a package\n",
    "# wd = Path(__file__).parent.parent.resolve()\n",
    "# sys.path.append(str(wd))\n",
    "# from apex.optimizers import FusedAdam #torch optimizer has a cuda backend, which is faster actually\n",
    "from lit_gpt.model import GPT, Block, Config, CausalSelfAttention\n",
    "from lit_gpt.packed_dataset import CombinedDataset, PackedDataset\n",
    "from lit_gpt.speed_monitor import SpeedMonitorFabric as Monitor\n",
    "from lit_gpt.speed_monitor import estimate_flops, measure_flops\n",
    "from lit_gpt.utils import chunked_cross_entropy, get_default_supported_precision, num_parameters, step_csv_logger, lazy_load\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from lit_gpt import FusedCrossEntropyLoss\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint\n",
    "\n",
    "model_name = \"tiny_LLaMA_120M_AttnSSM\"\n",
    "name = model_name\n",
    "out_dir = Path(\"out\") / name\n",
    "version = 4\n",
    "\n",
    "# Hyperparameters\n",
    "num_of_devices = 2\n",
    "global_batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "# micro_batch_size = 16\n",
    "micro_batch_size = 4\n",
    "max_step = 715256\n",
    "warmup_steps = 1000\n",
    "log_step_interval = 100\n",
    "eval_iters = 100\n",
    "save_step_interval = 20000\n",
    "eval_step_interval = 2000\n",
    "\n",
    "\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0\n",
    "decay_lr = True\n",
    "min_lr = 4e-5\n",
    "\n",
    "batch_size = global_batch_size // num_of_devices\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "assert gradient_accumulation_steps > 0\n",
    "warmup_iters = warmup_steps * gradient_accumulation_steps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "max_iters = max_step * gradient_accumulation_steps\n",
    "lr_decay_iters = max_iters\n",
    "log_iter_interval = log_step_interval * gradient_accumulation_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = Path(\"/home/aiops/wangsd/TinyLlama/data/mix_sample_combined\")\n",
    "val_data_dir = Path(\"/home/aiops/wangsd/TinyLlama/data/mix_sample_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msanderwang\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/aiops/wangsd/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treat all dataset equally by their size. If you want to use a different weight for a dataset, add it to the list with the weight.\n",
    "train_data_config = [\n",
    "    (\"train_ind\", 1.0),\n",
    "]\n",
    "\n",
    "val_data_config = [\n",
    "    (\"train_ind\", 1.0),\n",
    "]\n",
    "\n",
    "hparams = {k: v for k, v in locals().items() if isinstance(v, (int, float, str)) and not k.startswith(\"_\")}\n",
    "# logger = step_csv_logger(\"out\", name, flush_logs_every_n_steps=log_iter_interval)\n",
    "\n",
    "import os\n",
    "import wandb\n",
    "def read_key_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read().strip()\n",
    "key_file_path = '/home/aiops/wangsd/TinyLlama_3/wandb_key.txt'\n",
    "wandb_key = read_key_from_file(key_file_path)\n",
    "wandb.login(key=wandb_key)\n",
    "# wandb_logger = WandbLogger(name=\"tiny_llama_120M_SSM_O2_exp\", id=\"tiny_llama_120M_SSM_O2_exp\", project=\"Rebuttal\")\n",
    "# wandb_logger = WandbLogger(name=f\"{model_name}_v{version}\", id=f\"{model_name}_v{version}\", project=\"TL3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'tiny_LLaMA_120M_AttnSSM', 'name': 'tiny_LLaMA_120M_AttnSSM', 'version': 4, 'num_of_devices': 2, 'global_batch_size': 16, 'learning_rate': 0.001, 'micro_batch_size': 4, 'max_step': 715256, 'warmup_steps': 1000, 'log_step_interval': 100, 'eval_iters': 100, 'save_step_interval': 20000, 'eval_step_interval': 2000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'decay_lr': True, 'min_lr': 4e-05, 'batch_size': 8, 'gradient_accumulation_steps': 2, 'warmup_iters': 2000, 'max_iters': 1430512, 'lr_decay_iters': 1430512, 'log_iter_interval': 200}\n"
     ]
    }
   ],
   "source": [
    "precision = None\n",
    "tpu = False\n",
    "devices = 1\n",
    "\n",
    "precision = precision or get_default_supported_precision(training=True, tpu=tpu)\n",
    "\n",
    "if devices > 1:\n",
    "    if tpu:\n",
    "        # For multi-host TPU training, the device count for Fabric is limited to the count on a single host.\n",
    "        devices = \"auto\"\n",
    "        strategy = XLAStrategy(sync_module_states=False)\n",
    "    else:\n",
    "        strategy = FSDPStrategy(\n",
    "            auto_wrap_policy={Block},\n",
    "            activation_checkpointing_policy=None,\n",
    "            state_dict_type=\"full\",\n",
    "            limit_all_gathers=True,\n",
    "            cpu_offload=False,\n",
    "        )\n",
    "else:\n",
    "    strategy = \"auto\"\n",
    "\n",
    "# fabric = L.Fabric(devices=devices, strategy=strategy, precision=precision, loggers=[logger, wandb_logger])\n",
    "# fabric = L.Fabric(devices=devices, strategy=strategy, precision=precision, loggers=[wandb_logger])\n",
    "fabric = L.Fabric(devices=devices, strategy=strategy, precision=precision, loggers=[])\n",
    "fabric.print(hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataloader(\n",
    "    batch_size: int, block_size: int, data_dir: Path, fabric, shuffle: bool = True, seed: int = 12345, split=\"train\"\n",
    ") -> DataLoader:\n",
    "    datasets = []\n",
    "    data_config = train_data_config if split == \"train\" else val_data_config\n",
    "    for prefix, _ in data_config:\n",
    "        filenames = sorted(glob.glob(str(data_dir / f\"{prefix}*\")))\n",
    "        random.seed(seed)\n",
    "        random.shuffle(filenames)\n",
    "\n",
    "        dataset = PackedDataset(\n",
    "            filenames,\n",
    "            # n_chunks control the buffer size. \n",
    "            # Note that the buffer size also impacts the random shuffle\n",
    "            # (PackedDataset is an IterableDataset. So the shuffle is done by prefetch a buffer and shuffle the buffer)\n",
    "            n_chunks=8,\n",
    "            block_size=block_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed+fabric.global_rank,\n",
    "            num_processes=fabric.world_size,\n",
    "            process_rank=fabric.global_rank,\n",
    "        )\n",
    "        datasets.append(dataset)\n",
    "\n",
    "    if not datasets:\n",
    "        raise RuntimeError(\n",
    "            f\"No data found at {data_dir}. Make sure you ran prepare_redpajama.py to create the dataset.\"\n",
    "        )\n",
    "\n",
    "    weights = [weight for _, weight in data_config]\n",
    "    sum_weights = sum(weights)\n",
    "    weights = [el / sum_weights for el in weights]\n",
    "\n",
    "    combined_dataset = CombinedDataset(datasets=datasets, seed=seed, weights=weights)\n",
    "\n",
    "    return DataLoader(combined_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    batch_size: int,\n",
    "    block_size: int,\n",
    "    fabric,\n",
    "    train_data_dir: Path = Path(\"data/redpajama_sample\"),\n",
    "    val_data_dir: Optional[Path] = None,\n",
    "    seed: int = 12345,\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    # Increase by one because we need the next word as well\n",
    "    effective_block_size = block_size + 1\n",
    "    train_dataloader = create_dataloader(\n",
    "        batch_size=batch_size,\n",
    "        block_size=effective_block_size,\n",
    "        fabric=fabric,\n",
    "        data_dir=train_data_dir,\n",
    "        shuffle=True,\n",
    "        seed=seed,\n",
    "        split=\"train\"\n",
    "    )\n",
    "    val_dataloader = (\n",
    "        create_dataloader(\n",
    "            batch_size=batch_size,\n",
    "            block_size=effective_block_size,\n",
    "            fabric=fabric,\n",
    "            data_dir=val_data_dir,\n",
    "            shuffle=False,\n",
    "            seed=seed,\n",
    "            split=\"validation\"\n",
    "        )\n",
    "        if val_data_dir\n",
    "        else None\n",
    "    )\n",
    "    return train_dataloader, val_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 3412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with {'org': 'StatNLP-research', 'name': 'tiny_LLaMA_120M_AttnSSM', 'block_size': 2048, 'vocab_size': 32000, 'padding_multiple': 64, 'padded_vocab_size': 32000, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'n_ssm': 128, 'order': 2, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'FusedRMSNorm', 'norm_eps': 1e-05, '_mlp_class': 'LLaMAMLP', 'intermediate_size': 2048, 'condense_ratio': 1, 'time_mixer': 'attnssm', 'parameterization': 'best', 'retnet_dropout': (0.0,)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_head 12\n",
      "n_query_groups 12\n",
      "head_size 64\n",
      "config.n_embd 768\n",
      "In AttentionSSM, the parameterization used is best\n",
      "In AttentionSSM, the hidden dimension used is 2304\n",
      "n_head 12\n",
      "n_query_groups 12\n",
      "head_size 64\n",
      "config.n_embd 768\n",
      "In AttentionSSM, the parameterization used is best\n",
      "In AttentionSSM, the hidden dimension used is 2304\n",
      "n_head 12\n",
      "n_query_groups 12\n",
      "head_size 64\n",
      "config.n_embd 768\n",
      "In AttentionSSM, the parameterization used is best\n",
      "In AttentionSSM, the hidden dimension used is 2304\n",
      "n_head 12\n",
      "n_query_groups 12\n",
      "head_size 64\n",
      "config.n_embd 768\n",
      "In AttentionSSM, the parameterization used is best\n",
      "In AttentionSSM, the hidden dimension used is 2304\n",
      "n_head 12\n",
      "n_query_groups 12\n",
      "head_size 64\n",
      "config.n_embd 768\n",
      "In AttentionSSM, the parameterization used is best\n",
      "In AttentionSSM, the hidden dimension used is 2304\n",
      "n_head 12\n",
      "n_query_groups 12\n",
      "head_size 64\n",
      "config.n_embd 768\n",
      "In AttentionSSM, the parameterization used is best\n",
      "In AttentionSSM, the hidden dimension used is 2304\n",
      "n_head 12\n",
      "n_query_groups 12\n",
      "head_size 64\n",
      "config.n_embd 768\n",
      "In AttentionSSM, the parameterization used is best\n",
      "In AttentionSSM, the hidden dimension used is 2304\n",
      "n_head 12\n",
      "n_query_groups 12\n",
      "head_size 64\n",
      "config.n_embd 768\n",
      "In AttentionSSM, the parameterization used is best\n",
      "In AttentionSSM, the hidden dimension used is 2304\n",
      "n_head 12\n",
      "n_query_groups 12\n",
      "head_size 64\n",
      "config.n_embd 768\n",
      "In AttentionSSM, the parameterization used is best\n",
      "In AttentionSSM, the hidden dimension used is 2304\n",
      "n_head 12\n",
      "n_query_groups 12\n",
      "head_size 64\n",
      "config.n_embd 768\n",
      "In AttentionSSM, the parameterization used is best\n",
      "In AttentionSSM, the hidden dimension used is 2304\n",
      "n_head 12\n",
      "n_query_groups 12\n",
      "head_size 64\n",
      "config.n_embd 768\n",
      "In AttentionSSM, the parameterization used is best\n",
      "In AttentionSSM, the hidden dimension used is 2304\n",
      "n_head 12\n",
      "n_query_groups 12\n",
      "head_size 64\n",
      "config.n_embd 768\n",
      "In AttentionSSM, the parameterization used is best\n",
      "In AttentionSSM, the hidden dimension used is 2304\n",
      "Time to instantiate model: 0.36 seconds.\n",
      "Total parameters 403,120,896\n",
      "Resuming training from out/tiny_LLaMA_120M_AttnSSM/iter-440000-ckpt.pth\n"
     ]
    }
   ],
   "source": [
    "# resume\n",
    "resume = True\n",
    "\n",
    "monitor = Monitor(fabric, window_size=2, time_unit=\"seconds\", log_iter_interval=log_iter_interval)\n",
    "\n",
    "if fabric.global_rank == 0:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config = Config.from_name(model_name)\n",
    "\n",
    "train_dataloader, val_dataloader = create_dataloaders(\n",
    "    batch_size=micro_batch_size,\n",
    "    block_size=config.block_size,\n",
    "    fabric=fabric,\n",
    "    train_data_dir=train_data_dir,\n",
    "    val_data_dir=val_data_dir,\n",
    "    seed=3412,\n",
    ")\n",
    "if val_dataloader is None:\n",
    "    train_dataloader = fabric.setup_dataloaders(train_dataloader)\n",
    "else:\n",
    "    train_dataloader, val_dataloader = fabric.setup_dataloaders(train_dataloader, val_dataloader)\n",
    "\n",
    "fabric.seed_everything(3412)  # same seed for every process to init model (FSDP)\n",
    "\n",
    "fabric.print(f\"Loading model with {config.__dict__}\")\n",
    "t0 = time.perf_counter()\n",
    "with fabric.init_module(empty_init=(fabric.world_size > 1)):\n",
    "    model = GPT(config)\n",
    "    model.apply(partial(model._init_weights ,n_layer=config.n_layer))\n",
    "\n",
    "\n",
    "fabric.print(f\"Time to instantiate model: {time.perf_counter() - t0:.02f} seconds.\")\n",
    "fabric.print(f\"Total parameters {num_parameters(model):,}\")\n",
    "\n",
    "# wandb_logger.watch(model, log=\"all\", log_freq=1000)\n",
    "\n",
    "model = fabric.setup(model)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2), foreach=False\n",
    ")\n",
    "# optimizer = FusedAdam(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2),adam_w_mode=True)\n",
    "optimizer = fabric.setup_optimizers(optimizer)\n",
    "\n",
    "state = {\"model\": model, \"optimizer\": optimizer, \"hparams\": hparams, \"iter_num\": 0, \"step_count\": 0}\n",
    "\n",
    "if resume is True:\n",
    "    resume = sorted(out_dir.glob(\"*.pth\"))[-1]\n",
    "if resume :\n",
    "    fabric.print(f\"Resuming training from {resume}\")\n",
    "    fabric.load(resume, state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2304, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.attn.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 2304])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.proj.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalSelfAttentionSSM(\n",
       "  (attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "  (proj): Linear(in_features=2304, out_features=768, bias=False)\n",
       "  (hyenaSSM): SSM_Hyena(\n",
       "    (in_proj): Linear(in_features=2304, out_features=4608, bias=False)\n",
       "    (out_proj): Linear(in_features=2304, out_features=2304, bias=False)\n",
       "    (D): Linear(in_features=2304, out_features=2304, bias=False)\n",
       "    (filter_activation): GELU(approximate='none')\n",
       "    (activation): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
